---
title: "Practical Machine Learning Project"
author: "Sarfaraz Ahmed Qazi"
date: "`r Sys.Date()`"
output: html_document
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Overview
#This is the final project  the "Practical Machine Learning"  course authorized #by John Hopkins University through Coursera for their Data Science #Specialization. 

#It applies the machine learning techniques learnt in th course for predicting #the manner in which 6 participants did their physical activity whuch was #recorded by accelerometers on the belt, forearm, arm, and dumbell. This is the #“classe” variable in the training set to be predicted with 5 outcomes #A,B,C,D,and E. The outcome A is the correct way of exeuction while other four #outcomes are incorrect. We willselect four Machine Learning Algorithims viz. #Decision Trees,Random Forest, Support Vector Machines and Gradient Boosted #Trees using k-folds cross validation on the training set. The training set #will be divided into Training and Validation data and we will use the #performance metrics accuracy and out of sample error rate to decide the best #fitting model. Finally , we will use the best model to predict the #classification of the testcsv which contains 20 cases.


##Background
#Modeern acclomerator devides can record the large amount of data on the #personal physicial acitivity. A group of enthusiasts collected the data on #these mearementments to find patterns in their behavior i.e who much they do #their excercise. But the question is how well they do it. In this project, the #goal assigned is to use data and predict it. More information is available #from the website here: (http://groupware.les.inf.puc-rio.br/har) (see the #section on the Weight Lifting Exercise Dataset).

#The training data for this project are available here:

#https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

#The test data are available here:

#https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

#The data for this project come from this source: #http://web.archive.org/web/20161224072740/http:/grouW

#We acknowledge the courtesy of the owners/sponsors of the project and thankful #for their generosity to let us use their data for our academic purposes. 

# Loading required packags 
 

```{r message=FALSE}
library(rpart)
library(rpart.plot)
library(randomForest)
library(caret)
library(kernlab)
library(rattle)
library(corrplot)
```

# Loading data 


```{r}
traincsv <- read.csv("./data/pml-training.csv")
testcsv <- read.csv("./data/pml-testing.csv")

dim(traincsv)
dim(testcsv)
```
# Cleaning the Data

Removing irrelvent and unnecessary variables. 
```{r}
#Removing columns which are not related to model perfomance
training <- traincsv[,-c(1:7)] 
testing <- testcsv[,-c(1:7)] 
#Removing NA columns
training <- training[,colSums(is.na(training)) == 0]
testing <- testing[,colSums(is.na(testing)) == 0]
dim(training) 
dim(testing) 
```

#Now filtering variables which have near zero value.
```{r nzv}
NearZeroVar <- nearZeroVar(training)
training <- training[,-NearZeroVar]
dim(training)


```

#Correlation Analysis
After basic pre-processing and data cleaning, now we aee ready to prepare for our Machine Learning Venture. We start with correlation analysis of features. The dark colour circles show high correlations Red -ve  and Blue +ve. Not too much correlations. So we do not need PCA for dimentionality reduction.
```{r }
corMatrix <- cor(training[, -53])

#par(mar=c(1,1,1,1)),
corrplot(corMatrix,order = "AOE" , type = "upper",tl.cex = 0.4)
```




```{r}
inTrain <- createDataPartition(y=traincsv$classe, p=0.7, list=F)
DataTrain <- training[inTrain,]
DataValidation <- training[-inTrain,]
# dimension of original and training dataset
rbind("original dataset" = dim(traincsv),"training set" = dim(DataTrain))
```
# Models 
#We select the models Decision Trees, Random Forest, Support vector machines, #Gradient Boosted Trees and compare their performance metrics. 

## Decision Tree
#This is a simple model which uses branching like tree nodes to help predict  #classification problems.
  

```{r, cache=TRUE}
control_dt<- trainControl(method="cv", number=5, verboseIter=F)
set.seed(123)
ModelDT <- train(classe~., data=DataTrain, method="rpart", trControl = control_dt, tuneLength = 5)
fancyRpartPlot(ModelDT$finalModel)
predDT <- predict(ModelDT, DataValidation)
CM_DT <- confusionMatrix(predDT, factor(DataValidation$classe))
CM_DT
```
## Random Forest
#Random Forest is a set of decision trees its results are based on on the #outcomes of all its decision trees.It is a robust model. Let us see how its #results are better than decision tree model.  

```{r, cache=TRUE}
set.seed(123)
controlRf <- trainControl(method="cv", 5)
modelRf <- train(classe ~ ., data=DataTrain, method="rf", trControl=controlRf, ntree=100)
modelRf
predRF <- predict(modelRf, DataValidation)
CMRF <- confusionMatrix(predRF, factor(DataValidation$classe))
CMRF

```


#Support Vector Machines SVM
#The Random forest Model gives us probability of belonging to a class while the #Support vector machines gives the distance to the boundary of classification #to which a secific instance will belong. Depending upon the data #characteristics, SVM may perform better than Random Forests (which has already #high accuracy of 0.9932 on our data as calculated above). However, here the #SVM performs quite low than random forest 0.7813 but better than Decision #trees with accuracy of 0.5366

```{r, cache=TRUE}
set.seed(123)
controlSVM<- trainControl(method="cv", number=5, verboseIter=F)
modelSVM <- train(classe~., data=DataTrain, method="svmLinear", trControl = controlSVM, tuneLength = 5, verbose = F)

predSVM <- predict(modelSVM, DataValidation)
CM_SVM <- confusionMatrix(predSVM, factor(DataValidation$classe))
CM_SVM

```
## Gradient Boosted Machines GBM
#This is also ensamble method like Random Forests. But unlike Random Forest #which makes decisions trees and combines results of all trees at the #finalization of its process, GBM makes a decision tree at a time and learns to #improve the model while making another tree. Let us see how it performs in our #data. 


```{r, cache=TRUE}
control_gbm<- trainControl(method="cv", number=3, verboseIter=F)
modelGBM <- train(classe~., data=DataTrain, method="gbm", trControl = control_gbm, tuneLength = 5, verbose = F)

predGBM <- predict(modelGBM, DataValidation)
CM_GBM <- confusionMatrix(predGBM, factor(DataValidation$classe))
CM_GBM
```

##Comparison: The following is the final model performance summary:

#Decision tree: Accuracy:  0.5366		Out of Sample Error      0.4634
#Random forest: Accuracy:  0.9932	Out of Sample Error     0.0068
#SVM :	Accuracy:  0.7813		Out of Sample Error	0.2187
#GBM: Accuracy:  0.9908			Out of Sample Error     0.0092

#So Best Model is : Random Forest


#Let us See Plots:
#Decision trees

```{r, cache=TRUE }
plot(ModelDT)

```

```{r, cache=TRUE }
#random Forest
plot(modelRf)

```


` 

```{r, cache=TRUE }
#Gradient Boosting Model
plot(modelGBM)
```



#Let us see the performance of eh model on test data:

```{r, cache=TRUE }
PredictTestData <- predict(modelRf, newdata=testing)
PredictTestData

```
 
